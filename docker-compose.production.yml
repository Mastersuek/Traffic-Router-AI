version: '3.8'

services:
  # Main Application
  traffic-router-app:
    build:
      context: .
      dockerfile: Dockerfile.production
      args:
        NODE_ENV: production
    container_name: traffic-router-app-prod
    restart: unless-stopped
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=production
    env_file:
      - .env.production
    volumes:
      - ./logs:/var/log/traffic-router
      - ./cache:/app/cache
      - ./memory:/app/memory
    depends_on:
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
    networks:
      - traffic-router-network
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.25'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # AI Proxy Service
  ai-proxy:
    build:
      context: .
      dockerfile: Dockerfile.ai-proxy
    container_name: traffic-router-ai-proxy-prod
    restart: unless-stopped
    ports:
      - "3001:3001"
    environment:
      - NODE_ENV=production
    env_file:
      - .env.production
    volumes:
      - ./logs:/var/log/traffic-router
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - traffic-router-network
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.1'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Monitoring Service
  monitoring:
    build:
      context: .
      dockerfile: Dockerfile.monitoring
    container_name: traffic-router-monitoring-prod
    restart: unless-stopped
    ports:
      - "3002:3002"
      - "9090:9090"
    environment:
      - NODE_ENV=production
    env_file:
      - .env.production
    volumes:
      - ./logs:/var/log/traffic-router
      - ./monitoring-data:/app/monitoring-data
    networks:
      - traffic-router-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3002/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # YouTube Cache Service
  youtube-cache:
    build:
      context: .
      dockerfile: Dockerfile.youtube-cache
    container_name: traffic-router-youtube-cache-prod
    restart: unless-stopped
    ports:
      - "3003:3003"
    environment:
      - NODE_ENV=production
    env_file:
      - .env.production
    volumes:
      - ./logs:/var/log/traffic-router
      - ./youtube-cache:/app/youtube-cache
    networks:
      - traffic-router-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3003/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # MCP Server
  mcp-server:
    build:
      context: .
      dockerfile: Dockerfile.mcp-server
    container_name: traffic-router-mcp-server-prod
    restart: unless-stopped
    ports:
      - "3004:3004"
    environment:
      - NODE_ENV=production
    env_file:
      - .env.production
    volumes:
      - ./logs:/var/log/traffic-router
      - ./memory:/app/memory
    networks:
      - traffic-router-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3004/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Enhanced Recovery Agent
  recovery-agent:
    build:
      context: ./agents
      dockerfile: Dockerfile.recovery-agent
    container_name: traffic-router-recovery-agent-prod
    restart: unless-stopped
    environment:
      - PYTHON_ENV=production
    env_file:
      - .env.production
    volumes:
      - ./logs:/var/log/traffic-router
      - ./memory:/app/memory
      - ./agents:/app/agents
    depends_on:
      - traffic-router-app
      - ai-proxy
      - monitoring
    networks:
      - traffic-router-network
    healthcheck:
      test: ["CMD", "python", "-c", "import requests; requests.get('http://localhost:8000/health')"]
      interval: 60s
      timeout: 15s
      retries: 3

  # Redis Cache
  redis:
    image: redis:7-alpine
    container_name: traffic-router-redis-prod
    restart: unless-stopped
    ports:
      - "6379:6379"
    command: redis-server --appendonly yes --requirepass ${REDIS_PASSWORD}
    volumes:
      - redis-data:/data
    networks:
      - traffic-router-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  # PostgreSQL Database
  postgres:
    image: postgres:15-alpine
    container_name: traffic-router-postgres-prod
    restart: unless-stopped
    ports:
      - "5432:5432"
    environment:
      POSTGRES_DB: traffic_router_prod
      POSTGRES_USER: ${POSTGRES_USER:-traffic_router}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./database/init:/docker-entrypoint-initdb.d
    networks:
      - traffic-router-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-traffic_router}"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Nginx Reverse Proxy
  nginx:
    image: nginx:alpine
    container_name: traffic-router-nginx-prod
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.prod.conf:/etc/nginx/nginx.conf
      - ./nginx/ssl:/etc/nginx/ssl
      - ./logs/nginx:/var/log/nginx
    depends_on:
      - traffic-router-app
      - ai-proxy
      - monitoring
      - youtube-cache
    networks:
      - traffic-router-network
    healthcheck:
      test: ["CMD", "nginx", "-t"]
      interval: 30s
      timeout: 10s
      retries: 3

volumes:
  redis-data:
    driver: local
  postgres-data:
    driver: local

networks:
  traffic-router-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16