{
  "models": [
    {
      "alias": "gpt-4",
      "provider": "openai",
      "model": "gpt-4-turbo-preview",
      "endpoint": "https://api.openai.com/v1",
      "priority": 100,
      "enabled": true,
      "maxTokens": 4096,
      "temperature": 0.7,
      "timeout": 30000
    },
    {
      "alias": "gpt-3.5",
      "provider": "openai", 
      "model": "gpt-3.5-turbo",
      "endpoint": "https://api.openai.com/v1",
      "priority": 80,
      "enabled": true,
      "maxTokens": 4096,
      "temperature": 0.7,
      "timeout": 30000
    },
    {
      "alias": "claude-3",
      "provider": "anthropic",
      "model": "claude-3-opus-20240229",
      "endpoint": "https://api.anthropic.com/v1",
      "priority": 95,
      "enabled": true,
      "maxTokens": 4096,
      "temperature": 0.7,
      "timeout": 30000
    },
    {
      "alias": "claude-3-sonnet",
      "provider": "anthropic",
      "model": "claude-3-sonnet-20240229",
      "endpoint": "https://api.anthropic.com/v1",
      "priority": 85,
      "enabled": true,
      "maxTokens": 4096,
      "temperature": 0.7,
      "timeout": 30000
    },
    {
      "alias": "claude-3-haiku",
      "provider": "anthropic",
      "model": "claude-3-haiku-20240307",
      "endpoint": "https://api.anthropic.com/v1",
      "priority": 75,
      "enabled": true,
      "maxTokens": 4096,
      "temperature": 0.7,
      "timeout": 30000
    },
    {
      "alias": "gemini-pro",
      "provider": "google",
      "model": "gemini-1.5-pro",
      "endpoint": "https://generativelanguage.googleapis.com/v1beta",
      "priority": 90,
      "enabled": true,
      "maxTokens": 4096,
      "temperature": 0.7,
      "timeout": 30000
    },
    {
      "alias": "gemini-flash",
      "provider": "google",
      "model": "gemini-1.5-flash",
      "endpoint": "https://generativelanguage.googleapis.com/v1beta",
      "priority": 70,
      "enabled": true,
      "maxTokens": 4096,
      "temperature": 0.7,
      "timeout": 30000
    },
    {
      "alias": "llama-3.1-70b",
      "provider": "ollama",
      "model": "llama3.1:70b",
      "endpoint": "http://localhost:11434",
      "priority": 60,
      "enabled": false,
      "maxTokens": 4096,
      "temperature": 0.7,
      "timeout": 60000
    },
    {
      "alias": "llama-3.1-8b",
      "provider": "ollama",
      "model": "llama3.1:8b",
      "endpoint": "http://localhost:11434",
      "priority": 50,
      "enabled": false,
      "maxTokens": 4096,
      "temperature": 0.7,
      "timeout": 60000
    },
    {
      "alias": "codellama",
      "provider": "ollama",
      "model": "codellama:13b",
      "endpoint": "http://localhost:11434",
      "priority": 40,
      "enabled": false,
      "maxTokens": 4096,
      "temperature": 0.3,
      "timeout": 60000
    },
    {
      "alias": "mistral-7b",
      "provider": "ollama",
      "model": "mistral:7b",
      "endpoint": "http://localhost:11434",
      "priority": 30,
      "enabled": false,
      "maxTokens": 4096,
      "temperature": 0.7,
      "timeout": 60000
    },
    {
      "alias": "local-llm",
      "provider": "local",
      "model": "custom-model",
      "endpoint": "http://localhost:8000",
      "priority": 20,
      "enabled": false,
      "maxTokens": 2048,
      "temperature": 0.7,
      "timeout": 60000
    },
    {
      "alias": "llama-3.1-70b-together",
      "provider": "together",
      "model": "meta-llama/Llama-3.1-70B-Instruct-Turbo",
      "endpoint": "https://api.together.xyz",
      "apiKey": "${TOGETHER_AI_KEY}",
      "priority": 65,
      "enabled": false,
      "maxTokens": 4096,
      "temperature": 0.7,
      "timeout": 30000,
      "retryConfig": {
        "maxRetries": 2,
        "retryDelay": 1000,
        "backoffMultiplier": 2
      }
    },
    {
      "alias": "mixtral-8x7b-together",
      "provider": "together",
      "model": "mistralai/Mixtral-8x7B-Instruct-v0.1",
      "endpoint": "https://api.together.xyz",
      "apiKey": "${TOGETHER_AI_KEY}",
      "priority": 55,
      "enabled": false,
      "maxTokens": 4096,
      "temperature": 0.7,
      "timeout": 30000
    },
    {
      "alias": "llama-3.1-8b-groq",
      "provider": "groq",
      "model": "llama-3.1-8b-instant",
      "endpoint": "https://api.groq.com",
      "apiKey": "${GROQ_API_KEY}",
      "priority": 65,
      "enabled": false,
      "maxTokens": 4096,
      "temperature": 0.7,
      "timeout": 15000
    },
    {
      "alias": "mixtral-8x7b-groq",
      "provider": "groq",
      "model": "mixtral-8x7b-32768",
      "endpoint": "https://api.groq.com",
      "apiKey": "${GROQ_API_KEY}",
      "priority": 60,
      "enabled": false,
      "maxTokens": 32768,
      "temperature": 0.7,
      "timeout": 15000
    },
    {
      "alias": "command-r-cohere",
      "provider": "cohere",
      "model": "command-r",
      "endpoint": "https://api.cohere.ai",
      "apiKey": "${COHERE_API_KEY}",
      "priority": 45,
      "enabled": false,
      "maxTokens": 4096,
      "temperature": 0.7,
      "timeout": 30000
    },
    {
      "alias": "command-r-plus-cohere",
      "provider": "cohere",
      "model": "command-r-plus",
      "endpoint": "https://api.cohere.ai",
      "apiKey": "${COHERE_API_KEY}",
      "priority": 50,
      "enabled": false,
      "maxTokens": 4096,
      "temperature": 0.7,
      "timeout": 30000
    },
    {
      "alias": "llama-3.1-70b-hf",
      "provider": "huggingface",
      "model": "meta-llama/Meta-Llama-3.1-70B-Instruct",
      "endpoint": "https://api-inference.huggingface.co",
      "apiKey": "${HUGGINGFACE_API_KEY}",
      "priority": 35,
      "enabled": false,
      "maxTokens": 4096,
      "temperature": 0.7,
      "timeout": 60000
    },
    {
      "alias": "codellama-34b-hf",
      "provider": "huggingface",
      "model": "codellama/CodeLlama-34b-Instruct-hf",
      "endpoint": "https://api-inference.huggingface.co",
      "apiKey": "${HUGGINGFACE_API_KEY}",
      "priority": 25,
      "enabled": false,
      "maxTokens": 4096,
      "temperature": 0.3,
      "timeout": 60000
    },
    {
      "alias": "llama-3.1-70b-replicate",
      "provider": "replicate",
      "model": "meta/meta-llama-3.1-70b-instruct",
      "endpoint": "https://api.replicate.com",
      "apiKey": "${REPLICATE_API_TOKEN}",
      "priority": 30,
      "enabled": false,
      "maxTokens": 4096,
      "temperature": 0.7,
      "timeout": 120000
    }
  ],
  "fallback": {
    "enabled": true,
    "maxRetries": 3,
    "retryDelay": 1000,
    "fallbackOrder": [
      "gpt-3.5",
      "claude-3-haiku",
      "gemini-flash",
      "llama-3.1-8b-groq",
      "mixtral-8x7b-groq",
      "llama-3.1-70b-together",
      "command-r-cohere",
      "llama-3.1-8b"
    ]
  },
  "defaultModel": "gpt-4",
  "logLevel": "info"
}